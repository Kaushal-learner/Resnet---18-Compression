{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1aRo_ljlUsWrWI8cBsQPdEVBl794940MR","timestamp":1749310966262},{"file_id":"101FzfFINkvP8rtSH3fuAj42cWy3gLiiB","timestamp":1749310910662},{"file_id":"1bsIDup4Fwc5Aro-tj3G7Oap9BvvgU14k","timestamp":1744814905067}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["IMPORTING LIBRARIES FOR MODEL COMPRESSION AND DATA EXTRACTION"],"metadata":{"id":"JOXOFh1ZsGMN"}},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"aIcCoaK36w7i"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"M_Pu1KaIT0tv"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader, SubsetRandomSampler\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from sklearn.metrics import f1_score\n","from sklearn.cluster import KMeans\n","from sklearn.model_selection import StratifiedShuffleSplit\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm"]},{"cell_type":"markdown","source":["EXTRACTING DATA"],"metadata":{"id":"MhivqMInsRP3"}},{"cell_type":"code","source":["import zipfile\n","\n","# Unzip the dataset\n","zip_path = '/content/archive.zip'\n","extract_path = '/content/archive/'\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_path)\n","\n","print(\"âœ… Dataset extracted!\")\n"],"metadata":{"id":"tPo1mdW-4ywY","executionInfo":{"status":"ok","timestamp":1744889120072,"user_tz":-330,"elapsed":8405,"user":{"displayName":"Tam","userId":"04920513227943324290"}},"outputId":"0ccd9b4d-4c8c-45e4-d118-9253948eb16b","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Dataset extracted!\n"]}]},{"cell_type":"markdown","source":["COMPRESSING ORIGINAL BASELINE MODEL WITH STRUCTURED PRUNING,DYNAMIC QUANTIZATION,KNOWLEDGE DISTILLATION AND ARCHITECTURE OPTIMIZATION"],"metadata":{"id":"cv6KJPMMY5Rr"}},{"cell_type":"code","source":["# Install specific torch-pruning version\n","!pip install torch-pruning==0.2.7\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","import torch_pruning as tp\n","import copy\n","import os\n","import time\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","import json\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","cpu_device = torch.device(\"cpu\")\n","\n","# Step 1: Pruning\n","def prune_model(model, pruning_rate=0.2):\n","    print(\"Starting pruning...\")\n","    model = copy.deepcopy(model).to(device)\n","    model.eval()\n","    example_inputs = torch.randn(1, 3, 224, 224).to(device)\n","    try:\n","        DG = tp.DependencyGraph()\n","        print(\"Building dependency graph...\")\n","        DG.build_dependency(model, example_inputs=example_inputs)\n","        pruner = tp.pruner.MagnitudePruner(\n","            model,\n","            example_inputs=example_inputs,\n","            importance=tp.importance.MagnitudeImportance(p=1),\n","            iterative_steps=1,\n","            ch_sparsity=pruning_rate,\n","            ignored_layers=[model.fc]\n","        )\n","        print(\"Applying pruning...\")\n","        pruner.step()\n","        print(\"Verifying pruned model...\")\n","        with torch.no_grad():\n","            output = model(example_inputs)\n","            print(f\"Pruned model output shape: {output.shape}\")\n","    except Exception as e:\n","        print(f\"Pruning failed: {e}\")\n","        return model\n","    print(\"Pruning completed.\")\n","    return model.to(device)\n","\n","# Step 2: Dynamic Quantization\n","def quantize_model(model):\n","    print(\"Starting dynamic quantization...\")\n","    model.eval()\n","    try:\n","        quantized_model = torch.quantization.quantize_dynamic(\n","            model.to(\"cpu\"), {nn.Conv2d, nn.Linear}, dtype=torch.qint8\n","        )\n","    except Exception as e:\n","        print(f\"Dynamic quantization failed: {e}\")\n","        return model\n","    print(\"Dynamic quantization completed.\")\n","    return quantized_model\n","\n","# Step 3: Architecture Optimization (SlimResNet18)\n","class SlimResNet18(nn.Module):\n","    def  __init__(self, num_classes=100):\n","        super(SlimResNet18, self). __init__()\n","        base_model = models.resnet18(weights=\"IMAGENET1K_V1\")\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(base_model.layer1, in_channels=32, out_channels=32)\n","        self.layer2 = self._slim_layer(base_model.layer2, in_channels=32, out_channels=64)\n","        self.layer3 = self._slim_layer(base_model.layer3, in_channels=64, out_channels=128)\n","        self.layer4 = self._slim_layer(base_model.layer4, in_channels=128, out_channels=256)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(256, num_classes)\n","\n","    def _make_layer(self, layer, in_channels, out_channels):\n","        new_blocks = []\n","        for block in layer:\n","            downsample = None\n","            if block.downsample is not None or in_channels != out_channels:\n","                downsample = nn.Sequential(\n","                    nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=block.stride, bias=False),\n","                    nn.BatchNorm2d(out_channels)\n","                )\n","            new_block = models.resnet.BasicBlock(\n","                inplanes=in_channels,\n","                planes=out_channels,\n","                stride=block.stride,\n","                downsample=downsample\n","            )\n","            new_block.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=block.stride, padding=1, bias=False)\n","            new_block.bn1 = nn.BatchNorm2d(out_channels)\n","            new_block.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n","            new_block.bn2 = nn.BatchNorm2d(out_channels)\n","            new_blocks.append(new_block)\n","            in_channels = out_channels\n","        return nn.Sequential(*new_blocks)\n","\n","    def _slim_layer(self, layer, in_channels, out_channels):\n","        new_blocks = []\n","        for block in layer:\n","            downsample = None\n","            if block.downsample is not None or in_channels != out_channels:\n","                downsample = nn.Sequential(\n","                    nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=block.stride, bias=False),\n","                    nn.BatchNorm2d(out_channels)\n","                )\n","            new_block = models.resnet.BasicBlock(\n","                inplanes=in_channels,\n","                planes=out_channels,\n","                stride=block.stride,\n","                downsample=downsample\n","            )\n","            new_block.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=block.stride, padding=1, bias=False)\n","            new_block.bn1 = nn.BatchNorm2d(out_channels)\n","            new_block.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n","            new_block.bn2 = nn.BatchNorm2d(out_channels)\n","            new_blocks.append(new_block)\n","            in_channels = out_channels\n","        return nn.Sequential(*new_blocks)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","        return x\n","\n","# Compression pipeline (without training)\n","def compress_resnet18(num_classes=100):\n","    print(\"Creating SlimResNet18...\")\n","    student_model = SlimResNet18(num_classes=num_classes).to(device)\n","    print(\"Applying pruning...\")\n","    pruned_model = prune_model(student_model, pruning_rate=0.1)\n","    print(\"Testing forward pass...\")\n","    try:\n","        with torch.no_grad():\n","            dummy_input = torch.randn(1, 3, 224, 224).to(device)\n","            output = pruned_model(dummy_input)\n","            print(f\"Forward pass successful. Output shape: {output.shape}\")\n","    except Exception as e:\n","        print(f\"Forward pass failed: {e}\")\n","        raise e\n","    return pruned_model\n","\n","# Step 4: Knowledge Distillation\n","def distill_model(student_model, teacher_model, train_loader, val_loader, epochs=5):\n","    print(\"Starting distillation...\")\n","    teacher_model.eval()\n","    student_model.train()\n","    criterion = nn.KLDivLoss(reduction=\"batchmean\")\n","    cross_entropy = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n","\n","    alpha = 0.9\n","    temperature = 4.0\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            with torch.no_grad():\n","                teacher_outputs = teacher_model(inputs)\n","            student_outputs = student_model(inputs)\n","            loss_distill = criterion(\n","                torch.log_softmax(student_outputs / temperature, dim=1),\n","                torch.softmax(teacher_outputs / temperature, dim=1)\n","            ) * (temperature ** 2)\n","            loss_ce = cross_entropy(student_outputs, labels)\n","            loss = alpha * loss_distill + (1 - alpha) * loss_ce\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n","        val_accuracy = evaluate_accuracy(student_model, val_loader)\n","        print(f\"Validation Accuracy: {val_accuracy}%\")\n","    return student_model\n","\n","# Evaluation functions\n","def evaluate_accuracy(model, data_loader, eval_device=device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(eval_device), labels.to(eval_device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    return 100 * correct / total\n","\n","def measure_latency(model, data_loader, eval_device=device, num_runs=100):\n","    model.eval()\n","    latencies = []\n","    with torch.no_grad():\n","        for i, (inputs, _) in enumerate(data_loader):\n","            if i >= num_runs:\n","                break\n","            inputs = inputs.to(eval_device)\n","            start_time = time.time()\n","            _ = model(inputs)\n","            latencies.append((time.time() - start_time) * 1000)\n","    return np.mean(latencies)\n","\n","def get_model_size_mb(model, path=\"/content/temp_model.pth\"):\n","    torch.save(model.state_dict(), path)\n","    size_mb = os.path.getsize(path) / (1024 * 1024)\n","    os.remove(path)\n","    return size_mb\n","\n","# Training and evaluation pipeline\n","def train_and_evaluate(student_model):\n","    teacher_model = models.resnet50(weights=\"IMAGENET1K_V1\")\n","    teacher_model.fc = nn.Linear(teacher_model.fc.in_features, 100)\n","    teacher_model = teacher_model.to(device)\n","\n","    print(\"Training student model...\")\n","    distilled_model = distill_model(student_model, teacher_model, train_loader, val_loader, epochs=7)\n","\n","    print(\"Quantizing model...\")\n","    quantized_model = quantize_model(distilled_model)\n","\n","    print(\"Evaluating quantized model...\")\n","    try:\n","        accuracy = evaluate_accuracy(quantized_model, val_loader_cpu, eval_device=cpu_device)\n","        latency = measure_latency(quantized_model, val_loader_cpu, eval_device=cpu_device)\n","        size_mb = get_model_size_mb(quantized_model)\n","    except Exception as e:\n","        print(f\"Evaluation failed: {e}\")\n","        return quantized_model\n","\n","    print(f\"Compressed Model Metrics:\")\n","    print(f\"Accuracy: {accuracy}%\")\n","    print(f\"Latency: {latency}ms\")\n","    print(f\"Model Size: {size_mb}MB\")\n","\n","    return quantized_model\n","\n","# Load dataset\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","data_dir = \"/content/drive/MyDrive/archive_dataset\"\n","try:\n","    dataset = ImageFolder(root=data_dir, transform=transform)\n","    print(f\"Dataset loaded with {len(dataset.classes)} classes.\")\n","except Exception as e:\n","    print(f\"Dataset loading failed: {e}\")\n","    raise e\n","train_size = int(0.8 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n","val_loader_cpu = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2, pin_memory=False)\n","\n","# Run compression and training\n","try:\n","    print(\"Running Part 1: Compression\")\n","    model = compress_resnet18(num_classes=100)\n","    print(\"Running Part 2: Training and Evaluation\")\n","    compressed_model = train_and_evaluate(model)\n","except Exception as e:\n","    print(f\"Error: {e}\")\n","\n","# Commented code for saving model and metadata\n","\"\"\"\n","# Save the model and metadata\n","model_path = \"/content/compressed_resnet18.pth\"\n","torch.save(compressed_model.state_dict(), model_path)\n","\n","metadata = {\n","    \"model_format\": \"pytorch\",\n","    \"architecture\": \"slim_resnet18\",\n","    \"num_classes\": 100\n","}\n","with open(\"/content/metadata.json\", \"w\") as f:\n","    json.dump(metadata, f)\n","\n","# Export to TFLite\n","import tensorflow as tf\n","dummy_input = torch.randn(1, 3, 224, 224).to(device)\n","onnx_path = \"/content/model.onnx\"\n","torch.onnx.export(compressed_model, dummy_input, onnx_path, opset_version=11)\n","\n","!pip install onnx onnx-tf tf2onnx\n","from onnx_tf.backend import prepare\n","import onnx\n","onnx_model = onnx.load(onnx_path)\n","tf_rep = prepare(onnx_model)\n","tflite_path = \"/content/compressed_resnet18.tflite\"\n","tf_rep.export_graph(\"/content/tf_model\")\n","converter = tf.lite.TFLiteConverter.from_saved_model(\"/content/tf_model\")\n","tflite_model = converter.convert()\n","with open(tflite_path, \"wb\") as f:\n","    f.write(tflite_model)\n","\n","print(f\"TFLite model saved to {tflite_path}\")\n","\"\"\""],"metadata":{"id":"7FrHaoo8vQo3","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1744634193718,"user_tz":-330,"elapsed":1722267,"user":{"displayName":"Kaushal Singhania","userId":"07428443657067254372"}},"outputId":"07af1549-c0dd-4407-cb43-d38f76f9e173"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch-pruning==0.2.7 in /usr/local/lib/python3.11/dist-packages (0.2.7)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torch-pruning==0.2.7) (2.5.1+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torch-pruning==0.2.7) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torch-pruning==0.2.7) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torch-pruning==0.2.7) (3.0.2)\n","Dataset loaded with 100 classes.\n","Running Part 1: Compression\n","Creating SlimResNet18...\n","Applying pruning...\n","Starting pruning...\n","Building dependency graph...\n","Applying pruning...\n","Verifying pruned model...\n","Pruned model output shape: torch.Size([1, 100])\n","Pruning completed.\n","Testing forward pass...\n","Forward pass successful. Output shape: torch.Size([1, 100])\n","Running Part 2: Training and Evaluation\n","Training student model...\n","Starting distillation...\n","Epoch 1, Loss: 0.45246759516000745\n","Validation Accuracy: 28.475%\n","Epoch 2, Loss: 0.4202399163246155\n","Validation Accuracy: 41.525%\n","Epoch 3, Loss: 0.39106116822361947\n","Validation Accuracy: 49.3875%\n","Epoch 4, Loss: 0.36937774324417116\n","Validation Accuracy: 52.6625%\n","Epoch 5, Loss: 0.3507634290754795\n","Validation Accuracy: 55.35%\n","Epoch 6, Loss: 0.3336819490045309\n","Validation Accuracy: 59.8375%\n","Epoch 7, Loss: 0.31795470909774304\n","Validation Accuracy: 60.4375%\n","Quantizing model...\n","Starting dynamic quantization...\n","Dynamic quantization completed.\n","Evaluating quantized model...\n","Compressed Model Metrics:\n","Accuracy: 60.475%\n","Latency: 336.5901446342468ms\n","Model Size: 8.679010391235352MB\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\n# Save the model and metadata\\nmodel_path = \"/content/compressed_resnet18.pth\"\\ntorch.save(compressed_model.state_dict(), model_path)\\n\\nmetadata = {\\n    \"model_format\": \"pytorch\",\\n    \"architecture\": \"slim_resnet18\",\\n    \"num_classes\": 100\\n}\\nwith open(\"/content/metadata.json\", \"w\") as f:\\n    json.dump(metadata, f)\\n\\n# Export to TFLite\\nimport tensorflow as tf\\ndummy_input = torch.randn(1, 3, 224, 224).to(device)\\nonnx_path = \"/content/model.onnx\"\\ntorch.onnx.export(compressed_model, dummy_input, onnx_path, opset_version=11)\\n\\n!pip install onnx onnx-tf tf2onnx\\nfrom onnx_tf.backend import prepare\\nimport onnx\\nonnx_model = onnx.load(onnx_path)\\ntf_rep = prepare(onnx_model)\\ntflite_path = \"/content/compressed_resnet18.tflite\"\\ntf_rep.export_graph(\"/content/tf_model\")\\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"/content/tf_model\")\\ntflite_model = converter.convert()\\nwith open(tflite_path, \"wb\") as f:\\n    f.write(tflite_model)\\n\\nprint(f\"TFLite model saved to {tflite_path}\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["RECOMPRESSING MODEL RES-NET18 WITH STRUCTURED PRUNING AND KNOWLEDGE DISTILLATION"],"metadata":{"id":"jh6ph46ksh5d"}},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","import zipfile\n","import time\n","import torch.nn.utils.prune as prune\n","\n","# Set device to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Paths\n","MODEL_PATH = '/content/compressed_resnet18.pth'\n","ZIP_PATH = '/content/archive.zip'\n","EXTRACT_PATH = '/content/archive/'\n","\n","# Extract Dataset\n","with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n","    zip_ref.extractall(EXTRACT_PATH)\n","\n","print(\"âœ… Dataset extracted!\")\n","\n","# Load Dataset\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor()\n","])\n","train_dataset = torchvision.datasets.ImageFolder(root=EXTRACT_PATH, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n","\n","# Define Model\n","num_classes = 100\n","model = torchvision.models.resnet18(pretrained=False)\n","model.fc = nn.Linear(model.fc.in_features, num_classes)\n","\n","# Load Pretrained Weights Safely\n","checkpoint = torch.load(MODEL_PATH, map_location=device)\n","filtered_checkpoint = {k: v for k, v in checkpoint.items() if k in model.state_dict() and model.state_dict()[k].size() == v.size()}\n","model.load_state_dict(filtered_checkpoint, strict=False)\n","model.to(device)\n","\n","# Apply Structured Pruning (Conv2d, structured)\n","parameters_to_prune = []\n","for name, module in model.named_modules():\n","    if isinstance(module, nn.Conv2d):\n","        parameters_to_prune.append((module, 'weight'))\n","\n","for module, param in parameters_to_prune:\n","    prune.ln_structured(module, name=param, amount=0.4, n=2, dim=0)  # Structured channel pruning\n","    prune.remove(module, param)  # Make pruning permanent\n","\n","# Knowledge Distillation Setup\n","teacher_model = torchvision.models.resnet18(pretrained=True)\n","teacher_model.fc = nn.Linear(teacher_model.fc.in_features, num_classes)\n","teacher_model.to(device)\n","teacher_model.eval()\n","\n","criterion = nn.CrossEntropyLoss()\n","kd_loss = nn.KLDivLoss(reduction='batchmean')\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","# Training Loop with KD\n","for epoch in range(2):  # Light fine-tuning\n","    model.train()\n","    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/2]\")\n","    for images, labels in loop:\n","        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        with torch.no_grad():\n","            teacher_outputs = teacher_model(images)\n","\n","        loss = 0.7 * kd_loss(nn.functional.log_softmax(outputs / 4, dim=1),\n","                             nn.functional.softmax(teacher_outputs / 4, dim=1)) + \\\n","               0.3 * criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        loop.set_postfix(loss=loss.item())\n","\n","# Save Fine-Tuned Model\n","torch.save(model.state_dict(), '/content/fine_tuned_resnet18_pruned.pth')\n","\n","# Accuracy Evaluation\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f\"Final Accuracy on Training Data: {accuracy:.2f}%\")\n","\n","# Latency Test\n","input_sample = torch.randn(1, 3, 224, 224).to(device)\n","\n","# Warm-up\n","for _ in range(10):\n","    _ = model(input_sample)\n","\n","# Measure latency\n","start_time = time.time()\n","for _ in range(100):\n","    _ = model(input_sample)\n","avg_latency = (time.time() - start_time) / 100 * 1000\n","print(f\"Average Latency (ms): {avg_latency:.2f}\")\n","\n","# Convert to Half Precision for Compression\n","model.half()\n","model.eval()\n","example_input = torch.randn(1, 3, 224, 224).to(device).half()\n","traced_model = torch.jit.trace(model, example_input)\n","traced_model_path = \"/content/final_resnet18_compressed.pt\"\n","traced_model.save(traced_model_path)\n","compressed_size = os.path.getsize(traced_model_path) / 1e6\n","print(f\"Compressed Model Size (MB): {compressed_size:.2f}\")\n","\n","print(\"âœ… Model pruned, fine-tuned, compressed, and ready for submission!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W0gd8g2zm3Bg","executionInfo":{"status":"ok","timestamp":1744833056080,"user_tz":-330,"elapsed":416536,"user":{"displayName":"Tam","userId":"04920513227943324290"}},"outputId":"4c0204cb-44ee-49d1-d5ce-ee2986ffa9d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Dataset extracted!\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n","<ipython-input-1-92194c243b55>:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(MODEL_PATH, map_location=device)\n","/usr/local/lib/python3.11/dist-packages/torch/_utils.py:392: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  device=storage.device,\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Epoch [1/2]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [02:37<00:00,  7.96it/s, loss=0.688]\n","Epoch [2/2]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [02:34<00:00,  8.08it/s, loss=0.576]\n"]},{"output_type":"stream","name":"stdout","text":["Final Accuracy on Training Data: 45.88%\n","Average Latency (ms): 2.99\n","Model Size (MB): 44.99\n","âœ… Model pruned (structured), fine-tuned, accuracy calculated, and optimized for evaluation!\n"]}]},{"cell_type":"markdown","source":["USING HALF PRECISION TO REDUCE SIZE  "],"metadata":{"id":"w8zKfzw7su2N"}},{"cell_type":"code","source":["# Convert to Half Precision for Compression\n","model.half()\n","model.eval()\n","example_input = torch.randn(1, 3, 224, 224).to(device).half()\n","traced_model = torch.jit.trace(model, example_input)\n","traced_model_path = \"/content/final_resnet18_compressed.pt\"\n","traced_model.save(traced_model_path)\n","compressed_size = os.path.getsize(traced_model_path) / 1e6\n","print(f\"Compressed Model Size (MB): {compressed_size:.2f}\")\n","\n","print(\"âœ… Model pruned, fine-tuned, compressed, and ready for submission!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oqBnsFgaKcGc","executionInfo":{"status":"ok","timestamp":1744833295601,"user_tz":-330,"elapsed":926,"user":{"displayName":"Tam","userId":"04920513227943324290"}},"outputId":"c31224b3-d9c4-44c3-df1b-ad3f5759b0d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Compressed Model Size (MB): 22.64\n","âœ… Model pruned, fine-tuned, compressed, and ready for submission!\n"]}]},{"cell_type":"markdown","source":["EVALUATION METRIC FOR FINAL COMPRESSED MODEL"],"metadata":{"id":"39HCWunPs9Ui"}},{"cell_type":"code","source":["# Convert to Half Precision and Trace for Compression\n","model.half()\n","model.eval()\n","example_input = torch.randn(1, 3, 224, 224).to(device).half()\n","traced_model = torch.jit.trace(model, example_input)\n","traced_model_path = \"/content/final_resnet18_compressed.pt\"\n","traced_model.save(traced_model_path)\n","\n","# Reload Compressed Model for Consistent Metric Evaluation\n","loaded_model = torch.jit.load(traced_model_path, map_location=device)\n","loaded_model.eval()\n","\n","# Accuracy Evaluation after Compression\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        images = images.half()  # important for half-precision input\n","        outputs = loaded_model(images)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","accuracy = 100 * correct / total\n","\n","# Latency Test after Compression\n","input_sample = torch.randn(1, 3, 224, 224).to(device).half()\n","for _ in range(10):\n","    _ = loaded_model(input_sample)\n","\n","start_time = time.time()\n","for _ in range(100):\n","    _ = loaded_model(input_sample)\n","avg_latency = (time.time() - start_time) / 100 * 1000\n","\n","# Compressed Model Size\n","compressed_size = os.path.getsize(traced_model_path) / 1e6\n","\n","# Print all metrics together\n","print(\"\\nðŸ“Š Final Evaluation Metrics (Post-Compression):\")\n","print(f\"Final Accuracy on Training Data: {accuracy:.2f}%\")\n","print(f\"Average Latency (ms): {avg_latency:.2f}\")\n","print(f\"Compressed Model Size (MB): {compressed_size:.2f}\")\n","\n","print(\"âœ… Metrics now reflect the compressed model state for fair scoring!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1pbLQ8CdL7ib","executionInfo":{"status":"ok","timestamp":1744833681324,"user_tz":-330,"elapsed":85555,"user":{"displayName":"Tam","userId":"04920513227943324290"}},"outputId":"7405b5ee-27d9-4381-9ce3-7570b11e85aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ðŸ“Š Final Evaluation Metrics (Post-Compression):\n","Final Accuracy on Training Data: 45.89%\n","Average Latency (ms): 3.09\n","Compressed Model Size (MB): 22.64\n","âœ… Metrics now reflect the compressed model state for fair scoring!\n"]}]}]}